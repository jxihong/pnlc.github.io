<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Better LLM reasoning and planning via lightweight offline RL training.">
  <meta name="keywords" content="PNLC">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Planning without Search: Refining Frontier Models with Offline Goal-Conditioned Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Planning without Search: Refining Frontier Models with Offline Goal-Conditioned Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jxihong.github.io/joeyhong/">Joey Hong</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~anca/">Anca Dragan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>,
            </span
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley,</span>
            <span class="author-block"><sup>2</sup>Google Deepmind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/pipeline.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="pnlc">Planning with a Natural Language Critic (PLNC)</span> improves LLM reasoning and planning using offline goal-conditioned learning
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, 
            such as negotiation and persuasion, require additional long-horizon reasoning and planning.
          </p>
          <p>
            Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. 
            In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. 
            Furthermore, frontier models do not expose the APIs necessary to be trained in such manner. 
            As a result, modern methods to improve the reasoning of frontier LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. 
          </p>
          <p>
            To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to frontier models. 
            These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, 
            to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module 
            that facilitates decision-making in multi-turn interactions.
          </p>
          <p>We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both 
            RL fine-tuning and prompting methods while maintaining efficiency and scalability.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Approach -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>
        
        <!-- Motivation -->
        <h3 class="title is-4">Key Idea</h3>
        <div class="content has-text-justified">
          <p>
            Rather than directly fine-tuning frontier models with reinforcement learning, which is oftan inscalable, we augment LLMs 
            with a light-weight value function learned from offline data. The value function outputs a natural language value that assesses the 
            thoughts of a base LLM agent, and allows the the LLM agent to refine its thoughts. 
          </p>
          <p>
            Rather than traditional scalar values, our natural language values provide rich, interpretable information in the form of various 
            positive and negative future outcomes and their likelihoods of occurring.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video" autoplay muted preload playsinline width="75%">
            <source src="./static/videos/motivation.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br/>
        
        <!-- Training. -->
        <h3 class="title is-4">Training</h3>
        <div class="content has-text-justified">
          <p>
            Using offline RL, we train a goal-conditioned Q-function that outputs the likelihood of a goal occuring after following a specified thought at 
            some state. Note that by training over embeddings of thoughts, rather than raw environment actions, our Q-function is very light-weight and 
            quick to train (a 3-layer fully-connected network).
          </p>
        </div>
        <div class="content has-text-centered">
          <figure>
          <img src="./static/images/training.png"
                 class="training-image"
                 alt="Training pipeline."/>
           <figcaption>Schematic illustrating how goal-conditioned Q-function is trained.</figcaption>
           </figure>
        </div>
        <!-- Inference. -->
        <h3 class="title is-4">Inference</h3>
        <div class="content has-text-justified">
          <p>
            During inference, our goal-conditioned Q-function augments the reasoning capabilities of base LLM agents via a natural language critic. The critic 
            assesses future positive and negative outcomes and their probabilities. This evaluation can be used in an iteraitive self-refinement procedure that
            allows the LLM agent to improve its thoughts. Note that this is significantly quicker than competing methods that require tree search in order to evaluate
            actions. 
          </p>
        </div>
        <div class="content has-text-centered">
          <figure>
          <img src="./static/images/inference.png"
                 class="inference-image"
                 alt="Inference pipeline."/>
           <figcaption>Schematic illustrating how our natural language critic is used to allow an LLM agent to refine its thoughts during inference.</figcaption>
           </figure>
        </div>
      </div>
    </div>
    <!--/ Animation. -->


    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-centered">
          <figure>
          <img src="./static/images/benchmarks.png"
                 class="benchmarks-image"
                 alt="Benchmarks."/>
           <figcaption>Different LLM agent domains that we evaluate on.</figcaption>
           </figure>
        </div>
        
        <div class="content has-text-justified">
          <p>
            To demonstrate the effectiveness of <span class="pnlc">PNLC</span>, we evaluate our method on a variety of multi-turn LLM agent benchmark tasks: 
            web shopping, social deduction games, and persuasion.
          </p>
          <p>
            We compare against several prompting and RL fine-tuning baselines, including 
            <a href="https://arxiv.org/abs/2303.11366">Reflexion</a>,
            <a href="https://arxiv.org/abs/2310.04406">LATS</a>,
            <a href="https://arxiv.org/abs/2402.19446">ArCHer</a>, and
            <a href="https://arxiv.org/abs/2408.07199">Agent Q</a>.
          </p>
        </div>
        
        <div class="columns is-centered">

        <!-- Example 1. -->
        <div class="column">
          <div class="content">
            <h3 class="title is-3">Social Deduction Example</h2>
            <p>
              Our method enables the LLM agent to completely change its initial thought from accusing another player (which may reveal 
              the agent's identity), to acting agreeable and hiding their true intentions.
            </p>
            <img src="./static/images/ex1.png"
                 class="example-1-image"
                 alt="Example 1."/>
          </div>
        </div>
        <!--/ Example 1. -->

      <!-- Example 2. -->
      <div class="column">
        <h3 class="title is-3">Persuasion Example</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              In the difficult task of persuading a skeptical user to donate, our LLM agent realizes that they should directly tackle
              the user's skepticism by mentioning how the charity is held accountable, rather than their initial plan of talking more about
              the charity's accomplishments.
            </p>
          <img src="./static/images/ex2.png"
                 class="example-2-image"
                 alt="Example 2."/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Examples. -->
    <h3 class="title is-4">Results</h3>
        <div class="content has-text-centered">
          <figure>
          <img src="./static/images/results.png"
                 class="results-image"
                 alt="Results." 
                width="75%" height="75%"/>
           <figcaption>Evaluation of our PNLC policy against several LLM agent baselines.</figcaption>
           </figure>
        </div>
      </div>
    </div>
    <!--/ Experiments. -->

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">NeRFies website</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
